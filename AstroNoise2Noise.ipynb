{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f264797d",
   "metadata": {},
   "source": [
    "# CSBDeep & Noise2Noise an Astrophotography Example\n",
    "We use the [CSBDeep](http://csbdeep.bioimagecomputing.com/) training framework which was published with [CARE](https://www.nature.com/articles/s41592-018-0216-7). Furthermore we use the [Noise2Noise](https://arxiv.org/abs/1803.04189) training principle first described by Lehtinen _et al._\n",
    "\n",
    "Noise2Noise states that multiple noisy observation are sufficient to train content aware image restoration networks. In other words, no ground truth data is required. This opens the door to many new applications where the acquisition of ground truth data is impossible or very time consuming. \n",
    "\n",
    "In this example we will train a content aware image restoration network with multiple subs from an astrophotography session.\n",
    "The possibility of using this learning model for astrophotography is discussed by the authors in this video: https://www.youtube.com/watch?v=dcV0OfxjrPQ\n",
    "\n",
    "The example here is based on learnings taken from examples in the CSBDeep repo `examples` and from the basic Noise2Noise example provided by Tim-Oliver Buchholz in the repo [\n",
    "Noise2Noise-with-CSBDeep](https://github.com/juglab/Noise2Noise-with-CSBDeep). Additionally the CSBDeep team have provided an great video about their framework here: https://www.youtube.com/watch?v=ipp0mxfjhwY\n",
    "\n",
    "Other contributions have been from learnings acquired from various astro forum posts on the topic of deep learning:\n",
    "\n",
    "* https://www.cloudynights.com/topic/789780-deep-learning-denoising-for-astrophotography/\n",
    "* https://www.cloudynights.com/topic/657928-deep-learning-for-random-noise-attenuation/\n",
    "\n",
    "*Enhancements have been provided by Maria Pavlou as code changes to the base CDBDeep repo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509cfb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow reloading of CSBDeep modules following any code changes\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# A couple required imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361afc9",
   "metadata": {},
   "source": [
    "# Pre-processing Training Data and Save\n",
    "We have a stack of sub frames in the `data/astro/train` folder, acquired with the same optical train and equipment.\n",
    "\n",
    "All images have been aligned and calibrated and saved in the tiff format.\n",
    "\n",
    "The example data can be downloaded [here](https://1drv.ms/u/s!AvWEkn9Anb_Nq9Aw52Xs3LuYEcq_rg?e=EexXxL)\n",
    "\n",
    "Place the train images in the `data/astro/train` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5bcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Data Parameters:\n",
    "# Root Data path\n",
    "basepath=Path('data/astro')\n",
    "# Train Data path/s\n",
    "source_dirs=['train']\n",
    "# Image file pattern. Note: only formats supported by imread currently\n",
    "pattern='*.tiff'\n",
    "# Image patch size\n",
    "patchsize=64\n",
    "# Training data output savefile path & name\n",
    "training_data_name='TrainData_p64_nPERC'\n",
    "training_data_filename=training_data_name + '.npz'\n",
    "save_file=basepath/training_data_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30eeadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an estimate of the number of non-overlapping patches for the images we have, sampling from the first we find.\n",
    "sampleimage = imread(list((basepath/source_dirs[0]).glob(pattern))[0])\n",
    "n_patches_per_image=np.int(sampleimage.shape[0]/patchsize)*np.int(sampleimage.shape[1]/patchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268658ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csbdeep.data import RawData, create_patches, norm_percentiles, norm_reinhard\n",
    "from csbdeep.data import NoPreProcessor, ReinhardPreProcessor\n",
    "\n",
    "# Load image pairs for Noise2Noise processing, each image paired against every other at most once.\n",
    "raw_data = RawData.from_folder_n2n(basepath, source_dirs=source_dirs, axes='YXC', pattern=pattern, preprocessor=NoPreProcessor())\n",
    "\n",
    "# Create patch data from image pairs with parameters,\n",
    "# normalization set as norm_percentiles() by default, optionally set to None, norm_reinhard() or other custom\n",
    "X, Y, XY_axes = create_patches(\n",
    "    raw_data, \n",
    "    patch_size=(patchsize,patchsize,3),\n",
    "    normalization=norm_percentiles(),\n",
    "    n_patches_per_image=n_patches_per_image,\n",
    "    save_file=save_file,\n",
    "    patch_filter=None,\n",
    "    overlap=False)\n",
    "\n",
    "print('Source Data Shape =', X.shape)\n",
    "print('Target Data Shape =', Y.shape)\n",
    "print('Data Axes =', XY_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csbdeep.utils import plot_some\n",
    "\n",
    "# We can visualize the resulting patch input and target pairs.\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(32,4))\n",
    "    sl = slice(16*i, 16*(i+1)), 0\n",
    "    plot_some(X[sl],Y[sl],title_list=[np.arange(sl[0].start,sl[0].stop)])\n",
    "    plt.show()\n",
    "\n",
    "# Clear loaded data from memory, as an alternative axes order is required for subsequent steps.\n",
    "del X,Y,XY_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47711ff",
   "metadata": {},
   "source": [
    "# Load Training Data\n",
    "Here we load the training data from a save file created in earlier steps.\n",
    "As data is loaded we can also split the data into training `X,Y` and validation `X_val,Y_val` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962643dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csbdeep.io import load_training_data\n",
    "# Train/Validation split %\n",
    "validation_split=0.1\n",
    "# Load saved training data to memory from save file\n",
    "(X,Y), (X_val,Y_val), axes = load_training_data(save_file, validation_split=validation_split, verbose=True)\n",
    "\n",
    "print('Source Train Data Shape =', X.shape)\n",
    "print('Target Train Data Shape =', Y.shape)\n",
    "print('Source Val Data Shape =', X_val.shape)\n",
    "print('Target Val Data Shape =', Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc870d",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "# Configure and Train the Learning Model\n",
    "\n",
    "Here we configure the training model parameters. \n",
    "Training will be done for each color channel separately and saved with individual names based on:\n",
    "\n",
    "* `training_data_name`\n",
    "* `model_base_name`\n",
    "* `channel_name[i]`\n",
    "\n",
    "## Training debug tools\n",
    "\n",
    "You can monitor the progress during training with [TensorBoard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) by starting it from the current working directory:\n",
    "\n",
    "    $ tensorboard --logdir=.\n",
    "\n",
    "Then connect to [http://localhost:6006/](http://localhost:6006/) with your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the deep learning framework\n",
    "from csbdeep.models import Config, CARE\n",
    "\n",
    "# Since we are training each channel separately: n_channel_in, n_channel_out = 1, 1\n",
    "# Probabilistic training will be used as this yields better results\n",
    "# Reccomended that train_steps_per_epoch set at 200-400, and train_epochs at 100-200 for good results.\n",
    "config = Config('SYXC', n_channel_in=1, n_channel_out=1, unet_kern_size=3, probabilistic=True, train_steps_per_epoch=100, train_epochs=50)\n",
    "\n",
    "# Give a name for the model\n",
    "model_base_name = '_ASTRO_01_01'\n",
    "\n",
    "model_name = training_data_name + model_base_name\n",
    "channel_names=['R', 'G', 'B']\n",
    "for i, channel in enumerate(channel_names):\n",
    "    full_model_name = model_name + '_' + channel\n",
    "    \n",
    "    # Create the Learning Model from the CARE framework with configuration\n",
    "    model = CARE(config, name=full_model_name, basedir='models')\n",
    "    \n",
    "    # Train the model and capture history\n",
    "    history = model.train(\n",
    "        X[:,:,:,i,np.newaxis], \n",
    "        Y[:,:,:,i,np.newaxis], \n",
    "        (X_val[:,:,:,i,np.newaxis], Y_val[:,:,:,i,np.newaxis]))\n",
    "    \n",
    "    # Save the model\n",
    "    model.export_TF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2eadc",
   "metadata": {},
   "source": [
    "# Load and De-noise an Image \n",
    "\n",
    "Here we load an example image to de-noise using the trained set of RGB models.\n",
    "\n",
    "The image to be de-noised must be normalized in the same way as the training data.\n",
    "Then for each channel the model is used to predict the de-noised output and these are then saved as a single RGB image.\n",
    "\n",
    "The example test image used can be downloaded [here](https://1drv.ms/u/s!AvWEkn9Anb_Nq9Aw52Xs3LuYEcq_rg?e=EexXxL).\n",
    "\n",
    "Place the test image in the `data/astro/test` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a086c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the test file\n",
    "test_file_name='CrescentNebula-NoSt-Deep.tiff'\n",
    "\n",
    "testfilepath=basepath/'test'/test_file_name\n",
    "x = imread(testfilepath)\n",
    "testaxes = 'YX'\n",
    "\n",
    "print('Test Image size =', x.shape)\n",
    "print('Test Image axes =', testaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f547edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csbdeep.data import PercentileNormalizer, PadAndCropResizer, ReinhardNormalizer, NoNormalizer\n",
    "\n",
    "channel_names=['R', 'G', 'B']\n",
    "output_denoised = []\n",
    "for i, channel in enumerate(channel_names):\n",
    "    full_model_name = model_name + '_' + channel\n",
    "    \n",
    "    # Load the model for the specific channel\n",
    "    print(\"Loading model:\", full_model_name)\n",
    "    model = CARE(config=None, name=full_model_name, basedir='models')\n",
    "\n",
    "    # Predict/de-noise the image channel with the corresponding trained model\n",
    "    # Default PercentileNormalizer is used to match the normalization used to train the model\n",
    "    output_denoised.append(\n",
    "        model.predict(x[:,:,i],testaxes, normalizer=PercentileNormalizer(), resizer=PadAndCropResizer())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load astropy library for saving the de-noised image in fits format\n",
    "from astropy.io import fits\n",
    "\n",
    "output_file_name = model_name + '_RGB_' + Path(test_file_name).stem + '.fits'\n",
    "output_file_path = basepath/'test'/output_file_name\n",
    "hdu = fits.PrimaryHDU(output_denoised)\n",
    "hdul = fits.HDUList([hdu])\n",
    "hdul.info()\n",
    "hdul.writeto(output_file_path)\n",
    "print(\"Output file saved:\", output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
